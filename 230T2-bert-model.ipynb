{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model training, performance evaluation and sentiment prediction\n",
    "### 230T2 - Megha Raveendran, Renee Reynolds, Derui Chen, LoÃ¯c Diridollou, Shiva Kandi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46AeYe4HG89m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "fk-4Di6RQFch",
    "outputId": "bed87f10-07d9-4994-f10e-2668517403fa"
   },
   "source": [
    "# Importing the data and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkqCy5DkQzbE"
   },
   "outputs": [],
   "source": [
    "new_df1 = pd.read_csv(\"list_titles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A_9PQR3RGOe"
   },
   "outputs": [],
   "source": [
    "new_df1.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "new_df1 = new_df1.loc[:80000]\n",
    "new_df1 = new_df1.sample(frac = 1, random_state = np.random.RandomState(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ECIEPf5R9ZM"
   },
   "outputs": [],
   "source": [
    "new_df1['label'] = new_df1['Return'].apply(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imXxf4nLHFii"
   },
   "outputs": [],
   "source": [
    "X = new_df.Title.values\n",
    "y = new_df.label.values\n",
    "\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twirEBPMHIrb"
   },
   "outputs": [],
   "source": [
    "test_data = new_df1.loc[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSdnQqEQHWg7"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YlGJaDEQHduZ",
    "outputId": "c204076e-3bee-439f-a6c1-96254322befb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   One in 10 home loans is under water: Economy.com \n",
      "Processed:  One in 10 home loans is under water: Economy.com\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0\n",
    "print('Original: ', X[0])\n",
    "print('Processed: ', text_preprocessing(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "cc2ee7915e67445a95869032a6f05486",
      "2dfee3a66a514535abbb9571d58af6ea",
      "4d55f5bb07a04b3aa8251c9833ac5cf6",
      "cdd68f07da6745c0b0b33443f56d7a1a",
      "fdb9ca8200704b78b2ac8e27593eb41f",
      "45e03494921c478c80a777c1d086aafd",
      "40412089025441a0acad85694425d463",
      "eef20db043674a878c70fb7846606358"
     ]
    },
    "colab_type": "code",
    "id": "sRjS2goJHfEE",
    "outputId": "9cb91c8c-4305-4310-f82d-067a36bb1371"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2ee7915e67445a95869032a6f05486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent), add_special_tokens=True, max_length=MAX_LEN,             \n",
    "            pad_to_max_length=True, truncation = True, return_attention_mask=True)\n",
    "        \n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xgrDA56EHhQ-",
    "outputId": "497ac8c0-bfee-42d7-c015-7c9010f96bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  25\n"
     ]
    }
   ],
   "source": [
    "all_headlines = new_df.Title\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_headlines = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_headlines]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_headlines])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "E4I9USdMHo-X",
    "outputId": "756ceb81-776b-43c0-a96d-af1c72a096cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   One in 10 home loans is under water: Economy.com \n",
      "Token IDs:  [101, 2028, 1999, 2184, 2188, 10940, 2003, 2104, 2300, 1024, 4610, 1012, 4012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 64\n",
    "\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iy1NPWiIHrgG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dkLvRzWHHvCs",
    "outputId": "d39de797-7ce6-43b0-e28c-170611a3ff24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64 Âµs, sys: 0 ns, total: 64 Âµs\n",
      "Wall time: 67.9 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaPJS2moHxjc"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rgKZB_gHzjG"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "J3BuI34vH60v",
    "outputId": "bbdf211e-5276-422f-d5a6-ce0e7ff5cad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.699501   |     -      |     -     |   7.40   \n",
      "   1    |   40    |   0.690736   |     -      |     -     |   7.07   \n",
      "   1    |   60    |   0.687444   |     -      |     -     |   7.14   \n",
      "   1    |   80    |   0.696463   |     -      |     -     |   7.19   \n",
      "   1    |   100   |   0.693862   |     -      |     -     |   7.17   \n",
      "   1    |   120   |   0.691397   |     -      |     -     |   7.08   \n",
      "   1    |   140   |   0.692595   |     -      |     -     |   7.06   \n",
      "   1    |   160   |   0.693422   |     -      |     -     |   7.03   \n",
      "   1    |   180   |   0.694842   |     -      |     -     |   7.01   \n",
      "   1    |   200   |   0.691163   |     -      |     -     |   7.01   \n",
      "   1    |   220   |   0.689451   |     -      |     -     |   7.03   \n",
      "   1    |   240   |   0.691759   |     -      |     -     |   7.05   \n",
      "   1    |   260   |   0.694821   |     -      |     -     |   7.06   \n",
      "   1    |   280   |   0.694208   |     -      |     -     |   7.05   \n",
      "   1    |   300   |   0.688586   |     -      |     -     |   7.04   \n",
      "   1    |   320   |   0.692544   |     -      |     -     |   7.05   \n",
      "   1    |   340   |   0.693985   |     -      |     -     |   7.07   \n",
      "   1    |   360   |   0.688655   |     -      |     -     |   7.10   \n",
      "   1    |   380   |   0.692649   |     -      |     -     |   7.06   \n",
      "   1    |   400   |   0.690446   |     -      |     -     |   7.05   \n",
      "   1    |   420   |   0.686613   |     -      |     -     |   7.06   \n",
      "   1    |   440   |   0.680899   |     -      |     -     |   7.07   \n",
      "   1    |   460   |   0.690884   |     -      |     -     |   7.08   \n",
      "   1    |   480   |   0.691089   |     -      |     -     |   7.07   \n",
      "   1    |   500   |   0.690847   |     -      |     -     |   7.07   \n",
      "   1    |   520   |   0.692027   |     -      |     -     |   7.07   \n",
      "   1    |   540   |   0.692169   |     -      |     -     |   7.05   \n",
      "   1    |   560   |   0.689748   |     -      |     -     |   7.07   \n",
      "   1    |   580   |   0.681538   |     -      |     -     |   7.06   \n",
      "   1    |   600   |   0.685724   |     -      |     -     |   7.08   \n",
      "   1    |   620   |   0.687938   |     -      |     -     |   7.09   \n",
      "   1    |   640   |   0.687814   |     -      |     -     |   7.08   \n",
      "   1    |   660   |   0.690398   |     -      |     -     |   7.07   \n",
      "   1    |   680   |   0.688136   |     -      |     -     |   7.08   \n",
      "   1    |   700   |   0.692042   |     -      |     -     |   7.07   \n",
      "   1    |   720   |   0.684356   |     -      |     -     |   7.05   \n",
      "   1    |   740   |   0.677620   |     -      |     -     |   7.09   \n",
      "   1    |   760   |   0.695325   |     -      |     -     |   7.06   \n",
      "   1    |   780   |   0.689886   |     -      |     -     |   7.07   \n",
      "   1    |   800   |   0.686584   |     -      |     -     |   7.06   \n",
      "   1    |   820   |   0.684054   |     -      |     -     |   7.08   \n",
      "   1    |   840   |   0.685827   |     -      |     -     |   7.05   \n",
      "   1    |   860   |   0.677702   |     -      |     -     |   7.06   \n",
      "   1    |   880   |   0.681784   |     -      |     -     |   7.07   \n",
      "   1    |   900   |   0.686899   |     -      |     -     |   7.04   \n",
      "   1    |   920   |   0.685845   |     -      |     -     |   7.06   \n",
      "   1    |   940   |   0.684558   |     -      |     -     |   7.07   \n",
      "   1    |   960   |   0.683898   |     -      |     -     |   7.05   \n",
      "   1    |   980   |   0.677206   |     -      |     -     |   7.09   \n",
      "   1    |  1000   |   0.676621   |     -      |     -     |   7.09   \n",
      "   1    |  1020   |   0.695836   |     -      |     -     |   7.06   \n",
      "   1    |  1040   |   0.690211   |     -      |     -     |   7.03   \n",
      "   1    |  1060   |   0.689108   |     -      |     -     |   7.05   \n",
      "   1    |  1080   |   0.670090   |     -      |     -     |   7.05   \n",
      "   1    |  1100   |   0.671935   |     -      |     -     |   7.13   \n",
      "   1    |  1120   |   0.692587   |     -      |     -     |   7.09   \n",
      "   1    |  1140   |   0.693507   |     -      |     -     |   7.08   \n",
      "   1    |  1160   |   0.692744   |     -      |     -     |   7.02   \n",
      "   1    |  1180   |   0.692417   |     -      |     -     |   7.05   \n",
      "   1    |  1200   |   0.691255   |     -      |     -     |   7.04   \n",
      "   1    |  1220   |   0.683215   |     -      |     -     |   7.06   \n",
      "   1    |  1240   |   0.678589   |     -      |     -     |   7.08   \n",
      "   1    |  1260   |   0.686543   |     -      |     -     |   7.07   \n",
      "   1    |  1280   |   0.697419   |     -      |     -     |   7.04   \n",
      "   1    |  1300   |   0.686147   |     -      |     -     |   7.07   \n",
      "   1    |  1320   |   0.696770   |     -      |     -     |   7.08   \n",
      "   1    |  1340   |   0.680970   |     -      |     -     |   7.09   \n",
      "   1    |  1360   |   0.669923   |     -      |     -     |   7.10   \n",
      "   1    |  1380   |   0.684876   |     -      |     -     |   7.10   \n",
      "   1    |  1400   |   0.699255   |     -      |     -     |   7.11   \n",
      "   1    |  1420   |   0.685771   |     -      |     -     |   7.07   \n",
      "   1    |  1440   |   0.692984   |     -      |     -     |   7.05   \n",
      "   1    |  1460   |   0.687218   |     -      |     -     |   7.04   \n",
      "   1    |  1480   |   0.686521   |     -      |     -     |   7.04   \n",
      "   1    |  1500   |   0.683402   |     -      |     -     |   7.04   \n",
      "   1    |  1520   |   0.689438   |     -      |     -     |   7.07   \n",
      "   1    |  1540   |   0.686964   |     -      |     -     |   7.06   \n",
      "   1    |  1560   |   0.679260   |     -      |     -     |   7.07   \n",
      "   1    |  1580   |   0.697645   |     -      |     -     |   7.09   \n",
      "   1    |  1600   |   0.689794   |     -      |     -     |   7.04   \n",
      "   1    |  1620   |   0.688367   |     -      |     -     |   7.04   \n",
      "   1    |  1640   |   0.689954   |     -      |     -     |   7.04   \n",
      "   1    |  1660   |   0.683005   |     -      |     -     |   7.07   \n",
      "   1    |  1680   |   0.683454   |     -      |     -     |   7.10   \n",
      "   1    |  1700   |   0.689494   |     -      |     -     |   7.11   \n",
      "   1    |  1720   |   0.685704   |     -      |     -     |   7.09   \n",
      "   1    |  1740   |   0.695483   |     -      |     -     |   7.10   \n",
      "   1    |  1760   |   0.690356   |     -      |     -     |   7.10   \n",
      "   1    |  1780   |   0.678255   |     -      |     -     |   7.10   \n",
      "   1    |  1800   |   0.689864   |     -      |     -     |   7.10   \n",
      "   1    |  1820   |   0.673455   |     -      |     -     |   7.12   \n",
      "   1    |  1840   |   0.678976   |     -      |     -     |   7.12   \n",
      "   1    |  1860   |   0.682106   |     -      |     -     |   7.11   \n",
      "   1    |  1880   |   0.688052   |     -      |     -     |   7.11   \n",
      "   1    |  1900   |   0.685727   |     -      |     -     |   7.10   \n",
      "   1    |  1920   |   0.689547   |     -      |     -     |   7.08   \n",
      "   1    |  1940   |   0.694306   |     -      |     -     |   7.10   \n",
      "   1    |  1960   |   0.682621   |     -      |     -     |   7.08   \n",
      "   1    |  1980   |   0.690811   |     -      |     -     |   7.08   \n",
      "   1    |  2000   |   0.691275   |     -      |     -     |   7.10   \n",
      "   1    |  2020   |   0.680660   |     -      |     -     |   7.07   \n",
      "   1    |  2040   |   0.687894   |     -      |     -     |   7.11   \n",
      "   1    |  2060   |   0.685027   |     -      |     -     |   7.11   \n",
      "   1    |  2080   |   0.685891   |     -      |     -     |   7.13   \n",
      "   1    |  2100   |   0.687841   |     -      |     -     |   7.12   \n",
      "   1    |  2120   |   0.677081   |     -      |     -     |   7.09   \n",
      "   1    |  2140   |   0.684004   |     -      |     -     |   7.09   \n",
      "   1    |  2160   |   0.690215   |     -      |     -     |   7.10   \n",
      "   1    |  2180   |   0.698555   |     -      |     -     |   7.10   \n",
      "   1    |  2200   |   0.691165   |     -      |     -     |   7.11   \n",
      "   1    |  2220   |   0.690759   |     -      |     -     |   7.08   \n",
      "   1    |  2240   |   0.685713   |     -      |     -     |   7.10   \n",
      "   1    |  2249   |   0.672938   |     -      |     -     |   3.20   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.687737   |  0.681388  |   56.19   |  826.10  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.681373   |     -      |     -     |   7.43   \n",
      "   2    |   40    |   0.674950   |     -      |     -     |   7.08   \n",
      "   2    |   60    |   0.659814   |     -      |     -     |   7.10   \n",
      "   2    |   80    |   0.670682   |     -      |     -     |   7.12   \n",
      "   2    |   100   |   0.687088   |     -      |     -     |   7.12   \n",
      "   2    |   120   |   0.691992   |     -      |     -     |   7.10   \n",
      "   2    |   140   |   0.672715   |     -      |     -     |   7.10   \n",
      "   2    |   160   |   0.666491   |     -      |     -     |   7.11   \n",
      "   2    |   180   |   0.684885   |     -      |     -     |   7.10   \n",
      "   2    |   200   |   0.678338   |     -      |     -     |   7.11   \n",
      "   2    |   220   |   0.667900   |     -      |     -     |   7.11   \n",
      "   2    |   240   |   0.681660   |     -      |     -     |   7.11   \n",
      "   2    |   260   |   0.686174   |     -      |     -     |   7.11   \n",
      "   2    |   280   |   0.688521   |     -      |     -     |   7.10   \n",
      "   2    |   300   |   0.682969   |     -      |     -     |   7.09   \n",
      "   2    |   320   |   0.684785   |     -      |     -     |   7.11   \n",
      "   2    |   340   |   0.672111   |     -      |     -     |   7.09   \n",
      "   2    |   360   |   0.681297   |     -      |     -     |   7.10   \n",
      "   2    |   380   |   0.685207   |     -      |     -     |   7.09   \n",
      "   2    |   400   |   0.670080   |     -      |     -     |   7.10   \n",
      "   2    |   420   |   0.677668   |     -      |     -     |   7.11   \n",
      "   2    |   440   |   0.681590   |     -      |     -     |   7.13   \n",
      "   2    |   460   |   0.686838   |     -      |     -     |   7.12   \n",
      "   2    |   480   |   0.674857   |     -      |     -     |   7.12   \n",
      "   2    |   500   |   0.680101   |     -      |     -     |   7.11   \n",
      "   2    |   520   |   0.673663   |     -      |     -     |   7.10   \n",
      "   2    |   540   |   0.683733   |     -      |     -     |   7.12   \n",
      "   2    |   560   |   0.663881   |     -      |     -     |   7.11   \n",
      "   2    |   580   |   0.680216   |     -      |     -     |   7.11   \n",
      "   2    |   600   |   0.676063   |     -      |     -     |   7.12   \n",
      "   2    |   620   |   0.669138   |     -      |     -     |   7.11   \n",
      "   2    |   640   |   0.666889   |     -      |     -     |   7.13   \n",
      "   2    |   660   |   0.673175   |     -      |     -     |   7.13   \n",
      "   2    |   680   |   0.677166   |     -      |     -     |   7.11   \n",
      "   2    |   700   |   0.675229   |     -      |     -     |   7.14   \n",
      "   2    |   720   |   0.681940   |     -      |     -     |   7.12   \n",
      "   2    |   740   |   0.647516   |     -      |     -     |   7.11   \n",
      "   2    |   760   |   0.667894   |     -      |     -     |   7.11   \n",
      "   2    |   780   |   0.685729   |     -      |     -     |   7.12   \n",
      "   2    |   800   |   0.652920   |     -      |     -     |   7.11   \n",
      "   2    |   820   |   0.670539   |     -      |     -     |   7.12   \n",
      "   2    |   840   |   0.655154   |     -      |     -     |   7.14   \n",
      "   2    |   860   |   0.678001   |     -      |     -     |   7.12   \n",
      "   2    |   880   |   0.691980   |     -      |     -     |   7.12   \n",
      "   2    |   900   |   0.667166   |     -      |     -     |   7.11   \n",
      "   2    |   920   |   0.673696   |     -      |     -     |   7.14   \n",
      "   2    |   940   |   0.666139   |     -      |     -     |   7.11   \n",
      "   2    |   960   |   0.658974   |     -      |     -     |   7.11   \n",
      "   2    |   980   |   0.664377   |     -      |     -     |   7.12   \n",
      "   2    |  1000   |   0.665410   |     -      |     -     |   7.13   \n",
      "   2    |  1020   |   0.636169   |     -      |     -     |   7.12   \n",
      "   2    |  1040   |   0.670417   |     -      |     -     |   7.13   \n",
      "   2    |  1060   |   0.675923   |     -      |     -     |   7.11   \n",
      "   2    |  1080   |   0.676648   |     -      |     -     |   7.12   \n",
      "   2    |  1100   |   0.668377   |     -      |     -     |   7.12   \n",
      "   2    |  1120   |   0.659153   |     -      |     -     |   7.12   \n",
      "   2    |  1140   |   0.666316   |     -      |     -     |   7.12   \n",
      "   2    |  1160   |   0.667278   |     -      |     -     |   7.14   \n",
      "   2    |  1180   |   0.676913   |     -      |     -     |   7.10   \n",
      "   2    |  1200   |   0.654240   |     -      |     -     |   7.11   \n",
      "   2    |  1220   |   0.660041   |     -      |     -     |   7.12   \n",
      "   2    |  1240   |   0.662502   |     -      |     -     |   7.13   \n",
      "   2    |  1260   |   0.657093   |     -      |     -     |   7.14   \n",
      "   2    |  1280   |   0.655960   |     -      |     -     |   7.12   \n",
      "   2    |  1300   |   0.657870   |     -      |     -     |   7.12   \n",
      "   2    |  1320   |   0.673928   |     -      |     -     |   7.11   \n",
      "   2    |  1340   |   0.668567   |     -      |     -     |   7.13   \n",
      "   2    |  1360   |   0.692187   |     -      |     -     |   7.11   \n",
      "   2    |  1380   |   0.663280   |     -      |     -     |   7.10   \n",
      "   2    |  1400   |   0.664702   |     -      |     -     |   7.10   \n",
      "   2    |  1420   |   0.656121   |     -      |     -     |   7.11   \n",
      "   2    |  1440   |   0.652832   |     -      |     -     |   7.14   \n",
      "   2    |  1460   |   0.658102   |     -      |     -     |   7.11   \n",
      "   2    |  1480   |   0.657673   |     -      |     -     |   7.12   \n",
      "   2    |  1500   |   0.626785   |     -      |     -     |   7.13   \n",
      "   2    |  1520   |   0.675173   |     -      |     -     |   7.13   \n",
      "   2    |  1540   |   0.665768   |     -      |     -     |   7.13   \n",
      "   2    |  1560   |   0.660776   |     -      |     -     |   7.11   \n",
      "   2    |  1580   |   0.652705   |     -      |     -     |   7.12   \n",
      "   2    |  1600   |   0.672405   |     -      |     -     |   7.11   \n",
      "   2    |  1620   |   0.655330   |     -      |     -     |   7.10   \n",
      "   2    |  1640   |   0.674739   |     -      |     -     |   7.12   \n",
      "   2    |  1660   |   0.653910   |     -      |     -     |   7.11   \n",
      "   2    |  1680   |   0.649330   |     -      |     -     |   7.11   \n",
      "   2    |  1700   |   0.669446   |     -      |     -     |   7.12   \n",
      "   2    |  1720   |   0.644322   |     -      |     -     |   7.12   \n",
      "   2    |  1740   |   0.666952   |     -      |     -     |   7.12   \n",
      "   2    |  1760   |   0.651685   |     -      |     -     |   7.11   \n",
      "   2    |  1780   |   0.670831   |     -      |     -     |   7.12   \n",
      "   2    |  1800   |   0.663953   |     -      |     -     |   7.11   \n",
      "   2    |  1820   |   0.664513   |     -      |     -     |   7.11   \n",
      "   2    |  1840   |   0.660496   |     -      |     -     |   7.11   \n",
      "   2    |  1860   |   0.645929   |     -      |     -     |   7.12   \n",
      "   2    |  1880   |   0.637396   |     -      |     -     |   7.10   \n",
      "   2    |  1900   |   0.668595   |     -      |     -     |   7.12   \n",
      "   2    |  1920   |   0.643810   |     -      |     -     |   7.11   \n",
      "   2    |  1940   |   0.670727   |     -      |     -     |   7.12   \n",
      "   2    |  1960   |   0.653308   |     -      |     -     |   7.12   \n",
      "   2    |  1980   |   0.655545   |     -      |     -     |   7.13   \n",
      "   2    |  2000   |   0.651000   |     -      |     -     |   7.12   \n",
      "   2    |  2020   |   0.660693   |     -      |     -     |   7.12   \n",
      "   2    |  2040   |   0.667930   |     -      |     -     |   7.10   \n",
      "   2    |  2060   |   0.655450   |     -      |     -     |   7.12   \n",
      "   2    |  2080   |   0.649013   |     -      |     -     |   7.12   \n",
      "   2    |  2100   |   0.664532   |     -      |     -     |   7.14   \n",
      "   2    |  2120   |   0.641353   |     -      |     -     |   7.11   \n",
      "   2    |  2140   |   0.657805   |     -      |     -     |   7.11   \n",
      "   2    |  2160   |   0.641102   |     -      |     -     |   7.13   \n",
      "   2    |  2180   |   0.651609   |     -      |     -     |   7.10   \n",
      "   2    |  2200   |   0.666848   |     -      |     -     |   7.10   \n",
      "   2    |  2220   |   0.664251   |     -      |     -     |   7.12   \n",
      "   2    |  2240   |   0.650120   |     -      |     -     |   7.14   \n",
      "   2    |  2249   |   0.686964   |     -      |     -     |   3.20   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.666563   |  0.660916  |   61.01   |  830.43  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.626032   |     -      |     -     |   7.46   \n",
      "   3    |   40    |   0.610509   |     -      |     -     |   7.11   \n",
      "   3    |   60    |   0.600505   |     -      |     -     |   7.10   \n",
      "   3    |   80    |   0.657344   |     -      |     -     |   7.10   \n",
      "   3    |   100   |   0.622332   |     -      |     -     |   7.12   \n",
      "   3    |   120   |   0.602247   |     -      |     -     |   7.15   \n",
      "   3    |   140   |   0.617645   |     -      |     -     |   7.11   \n",
      "   3    |   160   |   0.623531   |     -      |     -     |   7.12   \n",
      "   3    |   180   |   0.615576   |     -      |     -     |   7.12   \n",
      "   3    |   200   |   0.602578   |     -      |     -     |   7.12   \n",
      "   3    |   220   |   0.603851   |     -      |     -     |   7.14   \n",
      "   3    |   240   |   0.606488   |     -      |     -     |   7.15   \n",
      "   3    |   260   |   0.615919   |     -      |     -     |   7.16   \n",
      "   3    |   280   |   0.595950   |     -      |     -     |   7.15   \n",
      "   3    |   300   |   0.618943   |     -      |     -     |   7.11   \n",
      "   3    |   320   |   0.609325   |     -      |     -     |   7.09   \n",
      "   3    |   340   |   0.618763   |     -      |     -     |   7.11   \n",
      "   3    |   360   |   0.626886   |     -      |     -     |   7.09   \n",
      "   3    |   380   |   0.602147   |     -      |     -     |   7.08   \n",
      "   3    |   400   |   0.635296   |     -      |     -     |   7.13   \n",
      "   3    |   420   |   0.619058   |     -      |     -     |   7.12   \n",
      "   3    |   440   |   0.635191   |     -      |     -     |   7.15   \n",
      "   3    |   460   |   0.608848   |     -      |     -     |   7.17   \n",
      "   3    |   480   |   0.616866   |     -      |     -     |   7.14   \n",
      "   3    |   500   |   0.595270   |     -      |     -     |   7.13   \n",
      "   3    |   520   |   0.566924   |     -      |     -     |   7.12   \n",
      "   3    |   540   |   0.617259   |     -      |     -     |   7.11   \n",
      "   3    |   560   |   0.623371   |     -      |     -     |   7.11   \n",
      "   3    |   580   |   0.606263   |     -      |     -     |   7.10   \n",
      "   3    |   600   |   0.620748   |     -      |     -     |   7.09   \n",
      "   3    |   620   |   0.603707   |     -      |     -     |   7.10   \n",
      "   3    |   640   |   0.606279   |     -      |     -     |   7.10   \n",
      "   3    |   660   |   0.618425   |     -      |     -     |   7.11   \n",
      "   3    |   680   |   0.584302   |     -      |     -     |   7.14   \n",
      "   3    |   700   |   0.591748   |     -      |     -     |   7.14   \n",
      "   3    |   720   |   0.616374   |     -      |     -     |   7.16   \n",
      "   3    |   740   |   0.646532   |     -      |     -     |   7.12   \n",
      "   3    |   760   |   0.615055   |     -      |     -     |   7.13   \n",
      "   3    |   780   |   0.583928   |     -      |     -     |   7.14   \n",
      "   3    |   800   |   0.618813   |     -      |     -     |   7.13   \n",
      "   3    |   820   |   0.586047   |     -      |     -     |   7.12   \n",
      "   3    |   840   |   0.594864   |     -      |     -     |   7.13   \n",
      "   3    |   860   |   0.599214   |     -      |     -     |   7.13   \n",
      "   3    |   880   |   0.576354   |     -      |     -     |   7.12   \n",
      "   3    |   900   |   0.590476   |     -      |     -     |   7.13   \n",
      "   3    |   920   |   0.612927   |     -      |     -     |   7.14   \n",
      "   3    |   940   |   0.605103   |     -      |     -     |   7.15   \n",
      "   3    |   960   |   0.601425   |     -      |     -     |   7.11   \n",
      "   3    |   980   |   0.632784   |     -      |     -     |   7.13   \n",
      "   3    |  1000   |   0.591535   |     -      |     -     |   7.14   \n",
      "   3    |  1020   |   0.617498   |     -      |     -     |   7.14   \n",
      "   3    |  1040   |   0.609548   |     -      |     -     |   7.14   \n",
      "   3    |  1060   |   0.580560   |     -      |     -     |   7.10   \n",
      "   3    |  1080   |   0.621832   |     -      |     -     |   7.09   \n",
      "   3    |  1100   |   0.609037   |     -      |     -     |   7.10   \n",
      "   3    |  1120   |   0.597648   |     -      |     -     |   7.09   \n",
      "   3    |  1140   |   0.600546   |     -      |     -     |   7.12   \n",
      "   3    |  1160   |   0.604268   |     -      |     -     |   7.12   \n",
      "   3    |  1180   |   0.593870   |     -      |     -     |   7.13   \n",
      "   3    |  1200   |   0.598349   |     -      |     -     |   7.15   \n",
      "   3    |  1220   |   0.605964   |     -      |     -     |   7.17   \n",
      "   3    |  1240   |   0.611560   |     -      |     -     |   7.16   \n",
      "   3    |  1260   |   0.581181   |     -      |     -     |   7.14   \n",
      "   3    |  1280   |   0.578713   |     -      |     -     |   7.13   \n",
      "   3    |  1300   |   0.607877   |     -      |     -     |   7.13   \n",
      "   3    |  1320   |   0.566003   |     -      |     -     |   7.10   \n",
      "   3    |  1340   |   0.581027   |     -      |     -     |   7.11   \n",
      "   3    |  1360   |   0.611747   |     -      |     -     |   7.11   \n",
      "   3    |  1380   |   0.579899   |     -      |     -     |   7.12   \n",
      "   3    |  1400   |   0.634107   |     -      |     -     |   7.11   \n",
      "   3    |  1420   |   0.585428   |     -      |     -     |   7.15   \n",
      "   3    |  1440   |   0.607259   |     -      |     -     |   7.14   \n",
      "   3    |  1460   |   0.576827   |     -      |     -     |   7.16   \n",
      "   3    |  1480   |   0.605647   |     -      |     -     |   7.13   \n",
      "   3    |  1500   |   0.600804   |     -      |     -     |   7.15   \n",
      "   3    |  1520   |   0.604542   |     -      |     -     |   7.16   \n",
      "   3    |  1540   |   0.587465   |     -      |     -     |   7.12   \n",
      "   3    |  1560   |   0.572806   |     -      |     -     |   7.14   \n",
      "   3    |  1580   |   0.593081   |     -      |     -     |   7.14   \n",
      "   3    |  1600   |   0.588903   |     -      |     -     |   7.14   \n",
      "   3    |  1620   |   0.594685   |     -      |     -     |   7.12   \n",
      "   3    |  1640   |   0.566907   |     -      |     -     |   7.13   \n",
      "   3    |  1660   |   0.586105   |     -      |     -     |   7.11   \n",
      "   3    |  1680   |   0.579231   |     -      |     -     |   7.13   \n",
      "   3    |  1700   |   0.596666   |     -      |     -     |   7.12   \n",
      "   3    |  1720   |   0.584423   |     -      |     -     |   7.13   \n",
      "   3    |  1740   |   0.608030   |     -      |     -     |   7.11   \n",
      "   3    |  1760   |   0.582749   |     -      |     -     |   7.10   \n",
      "   3    |  1780   |   0.612357   |     -      |     -     |   7.12   \n",
      "   3    |  1800   |   0.600373   |     -      |     -     |   7.11   \n",
      "   3    |  1820   |   0.606409   |     -      |     -     |   7.12   \n",
      "   3    |  1840   |   0.581931   |     -      |     -     |   7.12   \n",
      "   3    |  1860   |   0.597942   |     -      |     -     |   7.11   \n",
      "   3    |  1880   |   0.581804   |     -      |     -     |   7.11   \n",
      "   3    |  1900   |   0.573051   |     -      |     -     |   7.10   \n",
      "   3    |  1920   |   0.588440   |     -      |     -     |   7.11   \n",
      "   3    |  1940   |   0.585664   |     -      |     -     |   7.12   \n",
      "   3    |  1960   |   0.623842   |     -      |     -     |   7.13   \n",
      "   3    |  1980   |   0.575058   |     -      |     -     |   7.12   \n",
      "   3    |  2000   |   0.594223   |     -      |     -     |   7.13   \n",
      "   3    |  2020   |   0.570180   |     -      |     -     |   7.11   \n",
      "   3    |  2040   |   0.584820   |     -      |     -     |   7.10   \n",
      "   3    |  2060   |   0.575359   |     -      |     -     |   7.12   \n",
      "   3    |  2080   |   0.591419   |     -      |     -     |   7.11   \n",
      "   3    |  2100   |   0.580820   |     -      |     -     |   7.14   \n",
      "   3    |  2120   |   0.621599   |     -      |     -     |   7.10   \n",
      "   3    |  2140   |   0.551902   |     -      |     -     |   7.14   \n",
      "   3    |  2160   |   0.568472   |     -      |     -     |   7.11   \n",
      "   3    |  2180   |   0.594852   |     -      |     -     |   7.14   \n",
      "   3    |  2200   |   0.550239   |     -      |     -     |   7.11   \n",
      "   3    |  2220   |   0.592701   |     -      |     -     |   7.11   \n",
      "   3    |  2240   |   0.597657   |     -      |     -     |   7.13   \n",
      "   3    |  2249   |   0.598749   |     -      |     -     |   3.20   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.600108   |  0.648768  |   64.62   |  831.52  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=4)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=4, evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding functions for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6wSlpxGIDjf"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OtIRQrFSI5cc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "enpusEOJILuU",
    "outputId": "2b8ae1aa-0705-4979-d9df-68b12004df5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6892\n",
      "Accuracy: 64.48%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "49H7goXgIPVB",
    "outputId": "14dd2579-1b96-492e-f3b9-d87cd333bbf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(test_data.Title)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cS4n4a3vKZbR",
    "outputId": "01d01a9f-62af-4247-c37f-3a79c5e989e6"
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "# Get predictions from the probabilities\n",
    "threshold = 0.4\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "print(\"Number of headlines predicted non-negative: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sDOAehCKbYL"
   },
   "outputs": [],
   "source": [
    "output = test_data#[preds==1]\n",
    "#list(output.sample(20).tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputting the data to be used in the analysis notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKhq4CWWKdO_"
   },
   "outputs": [],
   "source": [
    "output.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "882otGPdIKej"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(probs).to_csv('probs.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2dfee3a66a514535abbb9571d58af6ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40412089025441a0acad85694425d463": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45e03494921c478c80a777c1d086aafd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d55f5bb07a04b3aa8251c9833ac5cf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45e03494921c478c80a777c1d086aafd",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fdb9ca8200704b78b2ac8e27593eb41f",
      "value": 231508
     }
    },
    "cc2ee7915e67445a95869032a6f05486": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d55f5bb07a04b3aa8251c9833ac5cf6",
       "IPY_MODEL_cdd68f07da6745c0b0b33443f56d7a1a"
      ],
      "layout": "IPY_MODEL_2dfee3a66a514535abbb9571d58af6ea"
     }
    },
    "cdd68f07da6745c0b0b33443f56d7a1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eef20db043674a878c70fb7846606358",
      "placeholder": "â",
      "style": "IPY_MODEL_40412089025441a0acad85694425d463",
      "value": " 232k/232k [00:00&lt;00:00, 902kB/s]"
     }
    },
    "eef20db043674a878c70fb7846606358": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdb9ca8200704b78b2ac8e27593eb41f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
